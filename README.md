# ğŸ§  Understanding How Aya "Thinks"

> **Exploring how multilingual language models (specifically the Aya model) process language internally.**

---

<details>
<summary>ğŸ“š Table of Contents</summary>

- [Understanding How Aya "Thinks"](#ğŸ§ -understanding-how-aya-thinks)
- [ğŸ§­ Project Context](#ğŸ§­-project-context)
- [ğŸš€ Model Clarification](#ğŸš€-model-clarification)
- [âœ¨ Summary Conclusions](#âœ¨-summary-conclusions)
  - [1. Aya's English-First Bias (English-First Ratio)](#1-ayas-english-first-bias-english-first-ratio)
  - [2. Aya's Token Likelihoods (Cosine Similarity)](#2-ayas-token-likelihoods-cosine-similarity)
  - [3. Ayaâ€™s QA Performance (Translation Impact)](#3-ayas-qa-performance-translation-impact)
  - [4. Ayaâ€™s Sentiment Steering Effectiveness](#4-ayas-sentiment-steering-effectiveness)
  - [5. Aya's Factual Consistency Across Languages](#5-ayas-factual-consistency-across-languages)
- [âœ¨ Final Summary Conclusion](#âœ¨-final-summary-conclusion)
- [ğŸ—‚ï¸ Repository Structure](#ğŸ—‚ï¸-repository-structure)

</details>

---

## ğŸ§­ Project Context

This research is part of the **Cohere Expedition: Aya Program**, an initiative to explore and better understand the capabilities, behaviors, and multilingual reasoning of the Aya model.  
We investigate how Aya internally processes information across languages and whether English-centric reasoning is prevalent.

You can read our [ğŸ“„ Research Ideas and Framework here](https://docs.google.com/document/d/1F5JfcpT1whHLKkwHnCnAWsi6gJ1dTF_rdRq1q8bH_js/edit?usp=sharing) and [ğŸ“Š Slides](https://docs.google.com/presentation/d/1o1uNmBi6_8UBuPOvolVvwMJ8cr2A4cgcjIcx17PxX1E/edit?usp=sharing).

---

## ğŸš€ Model Clarification

All experiments in this project use the **[Aya Expanse](https://docs.cohere.com/docs/aya-expanse)** (`c4ai-aya-expanse-8b`) model developed by Cohere.

Aya Expanse is described as a **highly performant multilingual model** capable of working with **23 languages**, providing strong capabilities in generation, reasoning, and understanding across diverse linguistic inputs.

---

## âœ¨ Summary Conclusions

### 1. Aya's English-First Bias (English-First Ratio)
- Aya shows **low overall English-centric bias** (English-First Ratio = **15%**).
- Certain languages like **English** and **Santhali** still exhibit strong English-first processing.

> ğŸ¯ **Main Answer**: âŒ Aya does **not consistently "think" in English first**, *despite a small fraction of languages showing strong English-first processing*.

---

### 2. Aya's Token Likelihoods (Cosine Similarity)
- **Average cosine similarity** is **0.244**, showing moderate English similarity.
- **Only English** itself exhibits near-perfect similarity (cosine = **0.803**).

> ğŸ¯ **Main Answer**: âŒ Aya **does not** consistently process languages with English-like token distributions.

---

### 3. Ayaâ€™s QA Performance (Translation Impact)
- **Performance strongest in English**.
- Aya's multilingual capabilities might still require more **adaptation** and **refinement** for non-English languages.

> ğŸ¯ **Main Answer**: âœ… **Yes, translation improves Aya's QA performance** in the sense that **Aya performs better on English inputs**.

---

### 4. Ayaâ€™s Sentiment Steering Effectiveness
- **Stronger sentiment shifts in English**.
- Languages like **Telugu**, **Swahili** show poor steerability.

> ğŸ¯ **Main Answer**: âœ… Aya is **much more steerable in English**.

---

### 5. Aya's Factual Consistency Across Languages

Aya's factual knowledge **is not fully neutral** across languages. **Factual consistency** varies significantly, with an overall consistency rate of just **34.5%**. Additionally, **semantic similarity** between answers is **moderate**, with an average embedding similarity of **0.674**, but still shows **notable variation** across languages.

> ğŸ¯ **Main Answer**: âŒ Aya **does not** store factual knowledge in a fully language-neutral way.

---

Hereâ€™s the summary in well-structured English:

---

## âœ¨ Final Summary Conclusion

Aya exhibits **mixed English-first tendencies**, but overall, it does **not consistently rely on English** across all languages. While **English and some related languages** show stronger English-first processing, Aya demonstrates **adequate multilingual capabilities** for other languages.

* **English-first bias**: Aya shows a **low overall English-centric bias** (English-First Ratio = **15%**), with certain languages like **English** and **Santhali** displaying stronger English-first processing.
* **Token likelihoods**: The **average cosine similarity** is **0.244**, indicating moderate similarity to English. **Only English** itself exhibits near-perfect similarity (**0.803**).
* **QA performance**: Aya performs **best in English**, and while it shows promising multilingual capabilities, it still requires more **adaptation and refinement** for non-English languages.
* **Sentiment steering**: Aya is much **more steerable in English**, with stronger sentiment shifts. Languages like **Telugu** and **Swahili** show weaker steerability.
* **Factual consistency**: Aya's **factual knowledge is not fully neutral** across languages. **Factual consistency** varies, with an overall consistency rate of **34.5%**. Additionally, **semantic similarity** between answers is moderate (average similarity of **0.674**), though there is **notable variation** across languages.

> ğŸ§  **Final Verdict**: Aya **does not consistently think in English first**, though certain languages show English-centric behavior. While it handles many languages reasonably well, Ayaâ€™s **internal knowledge and behavior are not fully language-neutral**, with **notable performance and consistency differences across languages**.

---

## ğŸ—‚ï¸ Repository Structure

````python
ğŸ“ understanding-how-Aya-thinks
 â”œâ”€â”€ ğŸ“ experiments/              # Python scripts to run each experiment
 â”‚    â”œâ”€â”€ factual_knowledge_experiment.py
 â”‚    â”œâ”€â”€ first_n_tokens_experiment.py
 â”‚    â”œâ”€â”€ likelihood_cosine_similarity_experiment.py
 â”‚    â”œâ”€â”€ steering_experiment.py
 â”‚    â””â”€â”€ translation_experiment.py
 â”‚
 â”œâ”€â”€ ğŸ“ notebooks/                # Jupyter notebooks for analysis and visualization
 â”‚    â”œâ”€â”€ factual_knowledge_analysis.ipynb
 â”‚    â”œâ”€â”€ first_n_tokens_analysis.ipynb
 â”‚    â”œâ”€â”€ likelihood_cosine_similarity_analysis.ipynb
 â”‚    â”œâ”€â”€ steering_analysis.ipynb
 â”‚    â””â”€â”€ translation_analysis.ipynb
 â”‚
 â”œâ”€â”€ ğŸ“ results/                  # Output data generated from experiments
 â”‚    â”œâ”€â”€ flores200_first_n_tokens_results.csv
 â”‚    â”œâ”€â”€ flores200_likelihood_cosine_similarity_results.csv
 â”‚    â”œâ”€â”€ mlama_factual_knowledge_results.csv
 â”‚    â”œâ”€â”€ tydiqa_steering_results.csv
 â”‚    â””â”€â”€ xquad_translation_results.csv
 â”‚
 â”œâ”€â”€ .env                         # Environment variables (API keys, config)
 â”œâ”€â”€ .gitignore                   # Specifies intentionally untracked files to ignore
 â”œâ”€â”€ LICENSE                      # Project open-source license
 â”œâ”€â”€ README.md                    # Main documentation file
 â”œâ”€â”€ requirements.txt             # Python package requirements
````

---

<br>

<p align="center">
  Made with â¤ï¸ during <strong>Cohere Expedition: Aya Program</strong>
</p>