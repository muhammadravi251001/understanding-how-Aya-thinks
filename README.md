# ğŸ§  Understanding How Aya "Thinks"

> **Exploring how multilingual language models (specifically the Aya model) process language internally.**

---

<details>
<summary>ğŸ“š Table of Contents</summary>

- [Understanding How Aya "Thinks"](#ğŸ§ -understanding-how-aya-thinks)
- [ğŸ§­ Project Context](#ğŸ§­-project-context)
- [ğŸš€ Model Clarification](#ğŸš€-model-clarification)
- [âœ¨ Summary Conclusions](#âœ¨-summary-conclusions)
  - [1. Aya's English-First Bias (English-First Ratio)](#1-ayas-english-first-bias-english-first-ratio)
  - [2. Aya's Token Likelihoods (Cosine Similarity)](#2-ayas-token-likelihoods-cosine-similarity)
  - [3. Ayaâ€™s QA Performance (Translation Impact)](#3-ayas-qa-performance-translation-impact)
  - [4. Ayaâ€™s Sentiment Steering Effectiveness](#4-ayas-sentiment-steering-effectiveness)
  - [5. Aya's Factual Consistency Across Languages](#5-ayas-factual-consistency-across-languages)
- [âœ¨ Final Summary Conclusion](#âœ¨-final-summary-conclusion)
- [ğŸ—‚ï¸ Repository Structure](#ğŸ—‚ï¸-repository-structure)

</details>

---

## ğŸ§­ Project Context

This research is part of the **Cohere Expedition: Aya Program**, an initiative to explore and better understand the capabilities, behaviors, and multilingual reasoning of the Aya model.  
We investigate how Aya internally processes information across languages and whether English-centric reasoning is prevalent.

You can read our [ğŸ“„ Research Ideas and Framework here](https://docs.google.com/document/d/1F5JfcpT1whHLKkwHnCnAWsi6gJ1dTF_rdRq1q8bH_js/edit?usp=sharing).

---

## ğŸš€ Model Clarification

All experiments in this project use the **[Aya Expanse](https://docs.cohere.com/docs/aya-expanse)** (`c4ai-aya-expanse-8b`) model developed by Cohere.

Aya Expanse is described as a **highly performant multilingual model** capable of working with **23 languages**, providing strong capabilities in generation, reasoning, and understanding across diverse linguistic inputs.

---

## âœ¨ Summary Conclusions

### 1. Aya's English-First Bias (English-First Ratio)
- Aya shows **low overall English-centric bias** (English-First Ratio = **15%**).
- Certain languages like **English** and **Sundanese** still exhibit strong English-first processing.

> ğŸ¯ **Main Answer**: âœ… Aya thinks in English **for some languages**, but **not universally**.

---

### 2. Aya's Token Likelihoods (Cosine Similarity)
- **Average cosine similarity** is **0.244**, showing moderate English similarity.
- **Only English** itself exhibits near-perfect similarity (cosine = **1.000**).

> ğŸ¯ **Main Answer**: âŒ Aya **does not** consistently process languages with English-like token distributions.

---

### 3. Ayaâ€™s QA Performance (Translation Impact)
- **Performance strongest in English**.
- Translating questions into English **helps in some cases**, but **not always**.

> ğŸ¯ **Main Answer**: âš¡ Translation **sometimes** improves QA but is **not a silver bullet**.

---

### 4. Ayaâ€™s Sentiment Steering Effectiveness
- **Stronger sentiment shifts in English**.
- Languages like **Telugu**, **Swahili** show poor steerability.

> ğŸ¯ **Main Answer**: âœ… Aya is **much more steerable in English**.

---

### 5. Aya's Factual Consistency Across Languages
- Aya's factual knowledge **is not perfectly neutral** across languages.
- **Semantic similarity** is **good (average 0.741)** but with significant variation.

> ğŸ¯ **Main Answer**: âŒ Aya **does not** store factual knowledge completely neutrally.

---

## âœ¨ Final Summary Conclusion

Ayaâ€™s internal "thinking" displays **mixed English-first tendencies**, but the model is **not dominated by English across the board**.  
**English and related languages** show biases, yet Aya maintains **strong multilingual adaptation** for many other languages.

> ğŸ§  **Final Verdict**: âŒ Aya does **not consistently think in English first**, though **English-centric bias exists** for some languages.

---

## ğŸ—‚ï¸ Repository Structure

````python
ğŸ“ understanding-how-Aya-thinks
 â”œâ”€â”€ ğŸ“ experiments/              # Python scripts to run each experiment
 â”‚    â”œâ”€â”€ factual_knowledge_experiment.py
 â”‚    â”œâ”€â”€ first_n_tokens_experiment.py
 â”‚    â”œâ”€â”€ likelihood_cosine_similarity_experiment.py
 â”‚    â”œâ”€â”€ steering_experiment.py
 â”‚    â””â”€â”€ translation_experiment.py
 â”‚
 â”œâ”€â”€ ğŸ“ notebooks/                # Jupyter notebooks for analysis and visualization
 â”‚    â”œâ”€â”€ factual_knowledge_analysis.ipynb
 â”‚    â”œâ”€â”€ first_n_tokens_analysis.ipynb
 â”‚    â”œâ”€â”€ likelihood_cosine_similarity_analysis.ipynb
 â”‚    â”œâ”€â”€ steering_analysis.ipynb
 â”‚    â””â”€â”€ translation_analysis.ipynb
 â”‚
 â”œâ”€â”€ ğŸ“ results/                  # Output data generated from experiments
 â”‚    â”œâ”€â”€ flores200_first_n_tokens_results.csv
 â”‚    â”œâ”€â”€ flores200_likelihood_cosine_similarity_results.csv
 â”‚    â”œâ”€â”€ mlama_factual_knowledge_results.csv
 â”‚    â”œâ”€â”€ tydiqa_steering_results.csv
 â”‚    â””â”€â”€ xquad_translation_results.csv
 â”‚
 â”œâ”€â”€ .env                         # Environment variables (API keys, config)
 â”œâ”€â”€ .gitignore                   # Specifies intentionally untracked files to ignore
 â”œâ”€â”€ LICENSE                      # Project open-source license
 â”œâ”€â”€ README.md                    # Main documentation file
 â”œâ”€â”€ requirements.txt             # Python package requirements
````

---

<br>

<p align="center">
  Made with â¤ï¸ during <strong>Cohere Expedition: Aya Program</strong>
</p>